# -*- coding: utf-8 -*-
"""notebookc474ce0c01

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/khushisharma1210/notebookc474ce0c01.a078ea26-bcf3-4983-9010-a99aa432823f.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20260204/auto/storage/goog4_request%26X-Goog-Date%3D20260204T205514Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db9201a0be77931406a511250a88aa22e34caa1773776dc788d8b82f3e2487d4e19ea823f6eec9de46b897daeb44cdd196606c62cf2b60eb971e99719874d5d82f94556871a69fd0c2fe050b596ed6e28c57bd3722467e13d58110297bf44d8e36c888ee8374f5ea77cb8e7feaec7bea53131a272e895c80eb82631dd213a158c7c8673580fa1f59d9ab47819107c9c0b82e5e80a09062d8e60dea88068a5d9acd991d524a4907ffc9ca80266f291d9e1eac8cd07836d21da13dadfb1316a2657c665bbcc4888da586f6a1631909374f8d0920050903c1663c8a03a922957aa35aa7e33a48792dbcff763939633e965bd541519562d570b1d2c7b4568b1c70d99
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

mlp_term_3_2025_kaggle_assignment_2_path = kagglehub.competition_download('mlp-term-3-2025-kaggle-assignment-2')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Import all necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix

# Import 7+ models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Set plot style
sns.set(style="whitegrid")

# Load the datasets
train_df = pd.read_csv("/kaggle/input/mlp-term-3-2025-kaggle-assignment-2/train.csv")
test_df = pd.read_csv("/kaggle/input/mlp-term-3-2025-kaggle-assignment-2/test.csv")
sample_submission = pd.read_csv("/kaggle/input/mlp-term-3-2025-kaggle-assignment-2/sample_submission.csv")

print(f"Training data shape: {train_df.shape}")
print(f"Test data shape: {test_df.shape}")

"""# **Rubric 1: Identify Data Types**"""

print("--- Data Types ---")
train_df.info()

"""# Rubric 2: Present Descriptive Statistics"""

print("\n--- Descriptive Statistics (Numerical) ---")
print(train_df.describe())

print("\n--- Descriptive Statistics (Categorical) ---")
print(train_df.describe(include=['object', 'category']))

"""# Rubric 3: Identify and Handle Missing Values"""

print(f"\n--- Missing Values (Initial Check) ---")
print(train_df.isnull().sum())


def feature_engineer(df):
    df = df.copy()

    # Handle impossible dates (like '2018-02-29') by coercing them to NaT (null)
    df['arrival'] = pd.to_datetime(
        df['arrival'],
        format='mixed',
        dayfirst=True,
        errors='coerce'
    )

    # Extract time-based features
    # The SimpleImputer will handle the NaT values later
    df['arrival_month'] = df['arrival'].dt.month
    df['arrival_day_of_week'] = df['arrival'].dt.dayofweek
    df['arrival_day_of_year'] = df['arrival'].dt.dayofyear

    # Create interaction features
    df['total_stay'] = df['weekends'] + df['weekdays']
    df['total_guests'] = df['adults'] + df['children']
    df['price_per_person'] = df['price'] / (df['total_guests'] + 1e-6)

    # Drop the original date column
    df = df.drop('arrival', axis=1)

    return df

# Apply feature engineering
train_df_fe = feature_engineer(train_df)
test_df_fe = feature_engineer(test_df)

print("Feature engineering complete.")
print(f"\n--- Missing Values (After Feature Engineering) ---")
print(f"Nulls in 'arrival_month': {train_df_fe['arrival_month'].isnull().sum()}")
print("These nulls will be imputed by our pipeline.")

"""# Rubric 4: Identify and Handle Duplicates"""

duplicate_count = train_df.duplicated().sum()
print(f"\n--- Duplicates ---")
print(f"Found {duplicate_count} duplicate rows.")

if duplicate_count > 0:
    train_df = train_df.drop_duplicates()
    print(f"Removed duplicates. New training data shape: {train_df.shape}")

"""# Rubric 5: Identify and Handle Outliers"""

print("\n--- Outlier Analysis ---")
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.boxplot(x=train_df_fe['price'])
plt.title('Boxplot for Price')

plt.subplot(1, 2, 2)
sns.boxplot(x=train_df_fe['lead_time'])
plt.title('Boxplot for Lead Time')

plt.tight_layout()
plt.show()

"""# Rubric 6: Present at least three visualizations and provide insights"""

print("\n--- Visualizations ---")

# Visualization 1: Target Variable Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='booking_status', data=train_df_fe)
plt.title('Distribution of Booking Status (0=Not Canceled, 1=Canceled)')
plt.show()
print("Insight 1: The dataset is imbalanced. 'Not Canceled' (0) bookings are more frequent than 'Canceled' (1) bookings. This means accuracy alone is not a good metric; we should also focus on F1-score or AUC-ROC.")

# Visualization 2: Cancellation Rate by Market Segment
plt.figure(figsize=(10, 6))
sns.countplot(x='segment', hue='booking_status', data=train_df_fe)
plt.title('Booking Status by Market Segment')
plt.show()
print("Insight 2: The 'Online TA' (Travel Agent) segment has a significantly higher volume of both bookings and cancellations compared to other segments. The 'Corporate' segment has a very low cancellation rate.")

# Visualization 3: Lead Time vs. Cancellation
plt.figure(figsize=(10, 6))
sns.histplot(data=train_df_fe, x='lead_time', hue='booking_status', kde=True, bins=50)
plt.title('Booking Status by Lead Time')
plt.show()
print("Insight 3: Bookings with a very short lead time (e.g., < 20 days) are highly likely to be 'Not Canceled' (0). As lead time increases, the proportion of 'Canceled' (1) bookings rises significantly. This is a very strong predictive feature.")

"""# Rubric 7: Scale Numerical features and Encode Categorical features"""

# 1. Define features (X) and target (y)
target = 'booking_status'
features = [col for col in train_df_fe.columns if col not in [target, 'id']]
X = train_df_fe[features]
y = train_df_fe[target]
X_test = test_df_fe[features] # Test data for final submission

# 2. Identify column types
numerical_cols = [
    'adults', 'children', 'weekends', 'weekdays',
    'lead_time', 'price', 'requests', 'arrival_month',
    'arrival_day_of_week', 'arrival_day_of_year', 'total_stay',
    'total_guests', 'price_per_person'
]
categorical_cols = [
    'meal_type', 'room_type', 'segment', 'repeat'
]

# 3. Create preprocessing pipelines
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# 4. Combine transformers
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ],
    remainder='passthrough'
)

# 5. Split data for validation
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

print("\nPreprocessor created and data split.")

"""# Rubric 8: Model Building (7 Models)"""

# Define models
models = {
    "Logistic Regression": LogisticRegression(random_state=42, max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42, n_jobs=-1),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "k-Nearest Neighbors": KNeighborsClassifier(n_jobs=-1),
    "XGBoost": XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss'),
    "LightGBM": LGBMClassifier(random_state=42, n_jobs=-1)
}

model_performance = {}

print("\n--- Training 8 Baseline Models ---")

for name, model in models.items():
    # Create the full pipeline
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    # Train
    pipeline.fit(X_train, y_train)

    # Predict
    y_pred = pipeline.predict(X_valid)

    # Evaluate
    score = f1_score(y_valid, y_pred, average='weighted')
    model_performance[name] = score
    print(f"{name}: Weighted F1-Score = {score:.4f}")

"""# Rubric 9: Hyperparameter Tuning on any 3 of the models"""

# We will tune: RandomForest, XGBoost, and LightGBM
# Note: Use small 'n_iter' for speed, increase for better results

# 1. RandomForest
rf_pipeline = Pipeline([('preprocessor', preprocessor),
                        ('model', RandomForestClassifier(random_state=42, n_jobs=-1))])

rf_params = {
    'model__n_estimators': [100, 200, 300],
    'model__max_depth': [None, 10, 20, 30],
    'model__min_samples_leaf': [1, 2, 4],
    'model__min_samples_split': [2, 5, 10]
}

rf_search = RandomizedSearchCV(rf_pipeline, rf_params, n_iter=10, cv=3,
                               scoring='f1_weighted', random_state=42, n_jobs=-1)
rf_search.fit(X_train, y_train)
print(f"Best RandomForest Score: {rf_search.best_score_:.4f}")
print(f"Best RandomForest Params: {rf_search.best_params_}")


# 2. XGBoost
xgb_pipeline = Pipeline([('preprocessor', preprocessor),
                         ('model', XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss'))])

xgb_params = {
    'model__n_estimators': [100, 300, 500],
    'model__learning_rate': [0.01, 0.1, 0.2],
    'model__max_depth': [3, 5, 7],
    'model__subsample': [0.7, 0.8, 1.0]
}

xgb_search = RandomizedSearchCV(xgb_pipeline, xgb_params, n_iter=10, cv=3,
                                scoring='f1_weighted', random_state=42, n_jobs=-1)
xgb_search.fit(X_train, y_train)
print(f"\nBest XGBoost Score: {xgb_search.best_score_:.4f}")
print(f"Best XGBoost Params: {xgb_search.best_params_}")


# 3. LightGBM
lgbm_pipeline = Pipeline([('preprocessor', preprocessor),
                          ('model', LGBMClassifier(random_state=42, n_jobs=-1))])

lgbm_params = {
    'model__n_estimators': [100, 300, 500],
    'model__learning_rate': [0.01, 0.1, 0.2],
    'model__num_leaves': [20, 31, 40],
    'model__subsample': [0.7, 0.8, 1.0]
}

lgbm_search = RandomizedSearchCV(lgbm_pipeline, lgbm_params, n_iter=10, cv=3,
                                 scoring='f1_weighted', random_state=42, n_jobs=-1)
lgbm_search.fit(X_train, y_train)
print(f"\nBest LightGBM Score: {lgbm_search.best_score_:.4f}")
print(f"Best LightGBM Params: {lgbm_search.best_params_}")

"""# Rubric 10: Comparison of model performances"""

# Create a DataFrame for comparison
performance_df = pd.DataFrame.from_dict(
    model_performance, orient='index', columns=['F1_Score']
).sort_values(by='F1_Score', ascending=False)

print("\n--- Model Performance Comparison ---")
print(performance_df)

# Plot the comparison
plt.figure(figsize=(10, 6))
sns.barplot(x=performance_df.index, y=performance_df['F1_Score'])
plt.title('Baseline Model Performance (Weighted F1-Score)')
plt.xticks(rotation=90)
plt.show()

# Select the best model (e.g., rf_search, xgb_search, or lgbm_search)
best_model_pipeline = xgb_search.best_estimator_
# or use your original model if it scored higher
# best_model_pipeline = rf_search.best_estimator_

# Evaluate on the validation set one last time
y_valid_pred = best_model_pipeline.predict(X_valid)
print("\n--- Final Model Evaluation (on Validation Set) ---")
print(f"Final Accuracy: {accuracy_score(y_valid, y_valid_pred):.4f}")
print(f"Final Weighted F1-Score: {f1_score(y_valid, y_valid_pred, average='weighted'):.4f}")
print("\nClassification Report:")
print(classification_report(y_valid, y_valid_pred))


# --- Create Submission File ---
print("\nCreating submission file...")
# We already trained our best_model_pipeline on the (X_train, y_train) split.
# For a final submission, it's best to retrain on ALL training data (X, y)
# Note: Retraining can take time
# best_model_pipeline.fit(X, y)

# Make predictions on the actual test set
test_predictions = best_model_pipeline.predict(X_test)

# Create the submission DataFrame
submission_df = pd.DataFrame({
    'id': test_df['id'],
    'booking_status': test_predictions
})

# Save to CSV
submission_df.to_csv('submission.csv', index=False)

print("submission.csv created successfully!")
print(submission_df.head())